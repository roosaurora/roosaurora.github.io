{"ast":null,"code":"var _jsxFileName = \"/Users/kallior/Desktop/porto/src/Build.js\";\nimport React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\nexport default class Analyse extends React.Component {\n  render() {\n    return /*#__PURE__*/React.createElement(\"div\", {\n      className: \"Section-wrapper\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 9,\n        columnNumber: 9\n      }\n    }, \"Every product is a prototype. While the design process shouldn't begin after putting a lot of effort in the implementation, it certainly doesn't end there. The final product of a sprint or a project always adds something new and unexpected, where lessons can be learned for future reference. Some of the more explorative HCI projects I found myself in during the college years were even based on this idea: the human-computer interaction can only be validated once fully experienced.\"), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 19,\n        columnNumber: 9\n      }\n    }, \"M:AV\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 20,\n        columnNumber: 9\n      }\n    }, \"This interactive art installation was born when we wanted to explore immersion with movement-based human-computer interaction. Body movement was mapped into digital sound and visuals using a Myo Gesture Control Armband and the Max software. During an open ended exhibition, we observed how people approached and immersed themselves in te installation and organically received valuable insights about the experience. These insights motivated a whole new round of ideation for our future explorations in the movement-based interaction domain.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"M:AV\",\n      src: \"https://www.youtube.com/embed/lAoymyRFVdg\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 30,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 34,\n        columnNumber: 9\n      }\n    }, \"Trash can-can\"), /*#__PURE__*/React.createElement(\"img\", {\n      src: require(\"./images/IoT_hackathon.jpg\"),\n      alt: \"Trash can\",\n      className: \"Image full right\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 35,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 40,\n        columnNumber: 9\n      }\n    }, \"During an IoT hackathon in France, our team did a 3-day design sprint with the following end result: an MVP for a self sorting trashcan. It was powered by Raspberry Pi and based on computer vision, using the OpenCV library for Python. During the live demo part of the pitch, which included us acting out the use scenarios, it could recognize up to 10 objects.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 48,\n        columnNumber: 9\n      }\n    }, \"Building this MVP really helped us to consider how using computer vision and code for self-sorting litter affets the design choices. While we could document what type of interaction the physical look and feel of the trash can encouraged, we also learned how crucial positioning the camera is and that gravity should be used when physically sorting trash into the sections corresponding to the categories defined in the code. We also learned how the realities of computer vision affect the possibilities of object categorisation.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 58,\n        columnNumber: 9\n      }\n    }, \"So, while a snappy pitch was prepared and the business model for the service approved by the jury, implementing this demo turned it into a feasible solution and a rewarding experience for our team.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Trash can-can\",\n      src: \"https://www.youtube.com/embed/cz3cQvwOsWA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 63,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 67,\n        columnNumber: 9\n      }\n    }, \"Mixed feelings\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 68,\n        columnNumber: 9\n      }\n    }, /*#__PURE__*/React.createElement(\"img\", {\n      src: require(\"./images/mixed_feelings.png\"),\n      alt: \"Mixed feelings\",\n      className: \"Image medium left\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 69,\n        columnNumber: 11\n      }\n    }), \"Exploring the magic of tangible interaction in mixed reality is hard to prototype in a very quick and dirty way. Me and my friend Joel did our best when sitting in the not-so-well air-conditioned homework room of our student accommodation building in France. Looking at the posters on the wall and drinking coke, we decided to create a portal to space and the ability to pass a virtual can of coke to a virtual astronaut by handing over the physical ones. The portal could be \\\"opened\\\" through a smartphone.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 83,\n        columnNumber: 9\n      }\n    }, \"Among with some other interactions, we implemented this with C# using the Unity engine with the Vuforia SDK. We put quite some effort into the interactive and 3-dimensional sound design of this reality, which you unfortunately can't appreciate in the following video which was filmed during an open exhibition, where headphones were used. You can, however, witness how the virtual characters respond to the tangible interaction of the user by passing through the portal, further seeming to bridge the gap between the two realities.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Mixed feelings\",\n      src: \"https://www.youtube.com/embed/1jA0qqfTutA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 93,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 97,\n        columnNumber: 9\n      }\n    }, \"Web apps\"), /*#__PURE__*/React.createElement(\"h5\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 98,\n        columnNumber: 9\n      }\n    }, \"BFAT\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 99,\n        columnNumber: 9\n      }\n    }, \"I market myself as a designer whose empathy doesn't end with the end user. Because I've done agile full-stack development, I'm familiar with the workflow defined by two screens, a keyboard and coffee breaks. BFAT3 was a test automation solution me and my team at Nokia built from scratch and released on time during a year between 2017 and 2018. It was implemented with React, Node.js and MongoDB.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 107,\n        columnNumber: 9\n      }\n    }, \"I learned a great deal about software architecture: the ways in which the GUI connects to and is, in fact, defined by the many data structures which go even beyond callbacks on the server side. On the other hand, I finally and fully realized how the GUI creates a user experience and how immensely useful a design system (a customized bootstrap library) was with the lack of a graphic designer next to me.\", \" \"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 115,\n        columnNumber: 9\n      }\n    }, \"I programmed the frontend and backend for a modular table-based UI and the interactions with the results of the software. Since then, I've come to realize that also other companies benefit from interactive tables which structure and modify data from several databases. I'm grateful to have digged deep into the topic and after learning about usability heuristics would indeed like to continue my work with the table design.\", \" \"), /*#__PURE__*/React.createElement(\"img\", {\n      src: require(\"./images/bfat.jpg\"),\n      alt: \"Bfat\",\n      className: \"Image medium\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 124,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h5\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 129,\n        columnNumber: 9\n      }\n    }, \"Hands and Omens website\"));\n  }\n\n}","map":{"version":3,"sources":["/Users/kallior/Desktop/porto/src/Build.js"],"names":["React","Analyse","Component","render","require"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,WAAP;AACA,OAAO,sCAAP;AAEA,eAAe,MAAMC,OAAN,SAAsBD,KAAK,CAACE,SAA5B,CAAsC;AACnDC,EAAAA,MAAM,GAAG;AACP,wBACE;AAAK,MAAA,SAAS,EAAC,iBAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6eADF,eAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAXF,eAYE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,siBAZF,eAsBE;AACE,MAAA,KAAK,EAAC,MADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAtBF,eA0BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBA1BF,eA2BE;AACI,MAAA,GAAG,EAAEC,OAAO,CAAC,4BAAD,CADhB;AAEI,MAAA,GAAG,EAAC,WAFR;AAGI,MAAA,SAAS,EAAC,kBAHd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MA3BF,eAgCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iXAhCF,eAwCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0hBAxCF,eAkDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+MAlDF,eAuDE;AACE,MAAA,KAAK,EAAC,eADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAvDF,eA2DE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBA3DF,eA4DE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AACE,MAAA,GAAG,EAAEA,OAAO,CAAC,6BAAD,CADd;AAEE,MAAA,GAAG,EAAC,gBAFN;AAGE,MAAA,SAAS,EAAC,mBAHZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADF,igBA5DF,eA2EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6hBA3EF,eAqFE;AACE,MAAA,KAAK,EAAC,gBADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MArFF,eAyFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAzFF,eA0FE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA1FF,eA2FE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uZA3FF,eAmGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gaAMyE,GANzE,CAnGF,eA2GE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kbAOgB,GAPhB,CA3GF,eAoHE;AACI,MAAA,GAAG,EAAEA,OAAO,CAAC,mBAAD,CADhB;AAEI,MAAA,GAAG,EAAC,MAFR;AAGI,MAAA,SAAS,EAAC,cAHd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MApHF,eAyHE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iCAzHF,CADF;AA6HD;;AA/HkD","sourcesContent":["import React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\n\nexport default class Analyse extends React.Component {\n  render() {\n    return (\n      <div className=\"Section-wrapper\">\n        <p>\n          Every product is a prototype. While the design process shouldn't begin\n          after putting a lot of effort in the implementation, it certainly\n          doesn't end there. The final product of a sprint or a project always\n          adds something new and unexpected, where lessons can be learned for\n          future reference. Some of the more explorative HCI projects I found\n          myself in during the college years were even based on this idea: the\n          human-computer interaction can only be validated once fully\n          experienced.\n        </p>\n        <h4>M:AV</h4>\n        <p>\n          This interactive art installation was born when we wanted to explore\n          immersion with movement-based human-computer interaction. Body\n          movement was mapped into digital sound and visuals using a Myo Gesture\n          Control Armband and the Max software. During an open ended exhibition,\n          we observed how people approached and immersed themselves in te\n          installation and organically received valuable insights about the\n          experience. These insights motivated a whole new round of ideation for\n          our future explorations in the movement-based interaction domain.\n        </p>\n        <iframe\n          title=\"M:AV\"\n          src=\"https://www.youtube.com/embed/lAoymyRFVdg\"\n        ></iframe>\n        <h4>Trash can-can</h4>\n        <img\n            src={require(\"./images/IoT_hackathon.jpg\")}\n            alt=\"Trash can\"\n            className=\"Image full right\"\n          />\n        <p>\n          During an IoT hackathon in France, our team did a 3-day design sprint\n          with the following end result: an MVP for a self sorting trashcan. It\n          was powered by Raspberry Pi and based on computer vision, using the\n          OpenCV library for Python. During the live demo part of the pitch,\n          which included us acting out the use scenarios, it could recognize up\n          to 10 objects.\n        </p>\n        <p>\n          Building this MVP really helped us to consider how using computer\n          vision and code for self-sorting litter affets the design choices.\n          While we could document what type of interaction the physical look and\n          feel of the trash can encouraged, we also learned how crucial\n          positioning the camera is and that gravity should be used when\n          physically sorting trash into the sections corresponding to the\n          categories defined in the code. We also learned how the realities of\n          computer vision affect the possibilities of object categorisation.\n        </p>\n        <p>\n          So, while a snappy pitch was prepared and the business model for the\n          service approved by the jury, implementing this demo turned it into a\n          feasible solution and a rewarding experience for our team.\n        </p>\n        <iframe\n          title=\"Trash can-can\"\n          src=\"https://www.youtube.com/embed/cz3cQvwOsWA\"\n        ></iframe>\n        <h4>Mixed feelings</h4>\n        <p>\n          <img\n            src={require(\"./images/mixed_feelings.png\")}\n            alt=\"Mixed feelings\"\n            className=\"Image medium left\"\n          />\n          Exploring the magic of tangible interaction in mixed reality is hard\n          to prototype in a very quick and dirty way. Me and my friend Joel did\n          our best when sitting in the not-so-well air-conditioned homework room\n          of our student accommodation building in France. Looking at the\n          posters on the wall and drinking coke, we decided to create a portal\n          to space and the ability to pass a virtual can of coke to a virtual\n          astronaut by handing over the physical ones. The portal could be\n          \"opened\" through a smartphone.\n        </p>\n        <p>\n          Among with some other interactions, we implemented this with C# using\n          the Unity engine with the Vuforia SDK. We put quite some effort into\n          the interactive and 3-dimensional sound design of this reality, which\n          you unfortunately can't appreciate in the following video which was\n          filmed during an open exhibition, where headphones were used. You can,\n          however, witness how the virtual characters respond to the tangible\n          interaction of the user by passing through the portal, further seeming\n          to bridge the gap between the two realities.\n        </p>\n        <iframe\n          title=\"Mixed feelings\"\n          src=\"https://www.youtube.com/embed/1jA0qqfTutA\"\n        ></iframe>\n        <h4>Web apps</h4>\n        <h5>BFAT</h5>\n        <p>\n          I market myself as a designer whose empathy doesn't end with the end\n          user. Because I've done agile full-stack development, I'm familiar\n          with the workflow defined by two screens, a keyboard and coffee\n          breaks. BFAT3 was a test automation solution me and my team at Nokia\n          built from scratch and released on time during a year between 2017 and\n          2018. It was implemented with React, Node.js and MongoDB.\n        </p>\n        <p>\n          I learned a great deal about software architecture: the ways in which\n          the GUI connects to and is, in fact, defined by the many data\n          structures which go even beyond callbacks on the server side. On the\n          other hand, I finally and fully realized how the GUI creates a user\n          experience and how immensely useful a design system (a customized\n          bootstrap library) was with the lack of a graphic designer next to me.{\" \"}\n        </p>\n        <p>\n          I programmed the frontend and backend for a modular table-based UI and\n          the interactions with the results of the software. Since then, I've\n          come to realize that also other companies benefit from interactive\n          tables which structure and modify data from several databases. I'm\n          grateful to have digged deep into the topic and after learning about\n          usability heuristics would indeed like to continue my work with the\n          table design.{\" \"}\n        </p>\n        <img\n            src={require(\"./images/bfat.jpg\")}\n            alt=\"Bfat\"\n            className=\"Image medium\"\n          />\n        <h5>Hands and Omens website</h5>\n      </div>\n    );\n  }\n}\n"]},"metadata":{},"sourceType":"module"}