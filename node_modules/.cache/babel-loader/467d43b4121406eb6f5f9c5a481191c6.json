{"ast":null,"code":"var _jsxFileName = \"/Users/kallior/Desktop/porto/src/Evaluate.js\";\nimport React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\nexport default class Analyse extends React.Component {\n  render() {\n    return /*#__PURE__*/React.createElement(\"div\", {\n      className: \"Section-wrapper\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 9,\n        columnNumber: 9\n      }\n    }, \"Evaluating a prototype, an interaction or full software must be one of the most challenging tasks of a designer. Defining the right methods for the intented purpose and making sense out of the results is not straight-forward and requires both extensive planning and all sorts of improvisaiton based on the situation. The design cycle represented by the navigation compass on this page will replay itself, as the analysis ties right into the results of each evaluation.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 18,\n        columnNumber: 9\n      }\n    }, \"In this section, I'll go through some of the evaluation methods that I've used during my projects and weren't described in the\", \" \", /*#__PURE__*/React.createElement(\"a\", {\n      href: \"#analyse\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 21,\n        columnNumber: 11\n      }\n    }, \"inquiry and analysis\"), \" section.\"), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 9\n      }\n    }, \"Guerrilla testing\"), /*#__PURE__*/React.createElement(\"img\", {\n      src: require(\"./images/cocat evaluation.png\"),\n      alt: \"Cocat evaluation\",\n      className: \"Image medium right\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 24,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 9\n      }\n    }, \"During a mobile development project we spontaneously asked college students, who fit our target group, to try out the current prototype with no instructions and explain out loud what they were thinking and trying to accomplish. We documented our observations and in the end, did a short user interview with each user. We used the results to improve the current prototype, which was always re-evaluated with new \\u2013\\xA0and some of the previous \\u2013\\xA0users.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 38,\n        columnNumber: 9\n      }\n    }, /*#__PURE__*/React.createElement(\"b\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 39,\n        columnNumber: 11\n      }\n    }, \"We chose the spontaenous guerrilla testing because we were designing for mobile\"), \", which should be simple and intuitive enough for first-time users. However, including some individual users for each iteration made the evaluation criteria more coherent and reliable. We also experimented with remote testing for the first time, which was benefical for accessing and documenting results. However, remotely interacting with a mobile screen simulator didn't necessarily give the same response from the users as proper interaction with the mobile interface would've. To minimize this effect, we only chose users who were already familiar with the concept to do remote evaluations.\"));\n  }\n\n}","map":{"version":3,"sources":["/Users/kallior/Desktop/porto/src/Evaluate.js"],"names":["React","Analyse","Component","render","require"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,WAAP;AACA,OAAO,sCAAP;AAEA,eAAe,MAAMC,OAAN,SAAsBD,KAAK,CAACE,SAA5B,CAAsC;AACnDC,EAAAA,MAAM,GAAG;AACP,wBACE;AAAK,MAAA,SAAS,EAAC,iBAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8dADF,eAUE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,yIAE4D,GAF5D,eAGE;AAAG,MAAA,IAAI,EAAC,UAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8BAHF,cAVF,eAeE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,2BAfF,eAgBE;AACE,MAAA,GAAG,EAAEC,OAAO,CAAC,+BAAD,CADd;AAEE,MAAA,GAAG,EAAC,kBAFN;AAGE,MAAA,SAAS,EAAC,oBAHZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAhBF,eAqBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wdArBF,eA8BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,yFADF,ulBA9BF,CADF;AAiDD;;AAnDkD","sourcesContent":["import React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\n\nexport default class Analyse extends React.Component {\n  render() {\n    return (\n      <div className=\"Section-wrapper\">\n        <p>\n          Evaluating a prototype, an interaction or full software must be one of\n          the most challenging tasks of a designer. Defining the right methods\n          for the intented purpose and making sense out of the results is not\n          straight-forward and requires both extensive planning and all sorts of\n          improvisaiton based on the situation. The design cycle represented by\n          the navigation compass on this page will replay itself, as the\n          analysis ties right into the results of each evaluation.\n        </p>\n        <p>\n          In this section, I'll go through some of the evaluation methods that\n          I've used during my projects and weren't described in the{\" \"}\n          <a href=\"#analyse\">inquiry and analysis</a> section.\n        </p>\n        <h4>Guerrilla testing</h4>\n        <img\n          src={require(\"./images/cocat evaluation.png\")}\n          alt=\"Cocat evaluation\"\n          className=\"Image medium right\"\n        />\n        <p>\n          During a mobile development project we spontaneously asked college\n          students, who fit our target group, to try out the current prototype\n          with no instructions and explain out loud what they were thinking and\n          trying to accomplish. We documented our observations and in the end,\n          did a short user interview with each user. We used the results to\n          improve the current prototype, which was always re-evaluated with new\n          – and some of the previous – users.\n        </p>\n        <p>\n          <b>\n            We chose the spontaenous guerrilla testing because we were designing\n            for mobile\n          </b>\n          , which should be simple and intuitive enough for first-time users.\n          However, including some individual users for each iteration made the\n          evaluation criteria more coherent and reliable. We also experimented\n          with remote testing for the first time, which was benefical for\n          accessing and documenting results. However, remotely interacting with\n          a mobile screen simulator didn't necessarily give the same response\n          from the users as proper interaction with the mobile interface\n          would've. To minimize this effect, we only chose users who were already\n          familiar with the concept to do remote evaluations.\n        </p>\n        \n      </div>\n    );\n  }\n}\n"]},"metadata":{},"sourceType":"module"}