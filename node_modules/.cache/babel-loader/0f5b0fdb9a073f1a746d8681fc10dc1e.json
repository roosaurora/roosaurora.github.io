{"ast":null,"code":"var _jsxFileName = \"/Users/kallior/Desktop/porto/src/Build.js\";\nimport React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\nexport default class Analyse extends React.Component {\n  render() {\n    return /*#__PURE__*/React.createElement(\"div\", {\n      className: \"Section-wrapper\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 9,\n        columnNumber: 9\n      }\n    }, \"Every product is a prototype. While the design process shouldn't begin after putting a lot of effort in the implementation, it certainly doesn't end there. The final product of a sprint or a project always adds something new and unexpected, where lessons can be learned for future reference. Some of the more explorative HCI projects I found myself in during the college years were even based on this idea: the human-computer interaction can only be validated once fully experienced.\"), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 19,\n        columnNumber: 9\n      }\n    }, \"M:AV\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 20,\n        columnNumber: 9\n      }\n    }, \"This interactive art installation was born when we wanted to explore immersion with movement-based human-computer interaction. Body movement was mapped into digital sound and visuals using a Myo Gesture Control Armband and the Max software. During an open ended exhibition, we observed how people approached and immersed themselves in te installation and organically received valuable insights about the experience. These insights motivated a whole new round of ideation for our future explorations in the movement-based interaction domain.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"M:AV\",\n      src: \"https://www.youtube.com/embed/lAoymyRFVdg\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 30,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 34,\n        columnNumber: 9\n      }\n    }, \"Trash can-can\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 35,\n        columnNumber: 9\n      }\n    }, \"During an IoT hackathon in France, our team did a 3-day design sprint with the following end result: an MVP for a self sorting trashcan. It was powered by Raspberry Pi and based on computer vision, using the OpenCV library for Python. During the live demo part of the pitch, which included us acting out the use scenarios, it could recognize up to 10 objects.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 43,\n        columnNumber: 9\n      }\n    }, \"Building this MVP really helped us to consider how using computer vision and code for self-sorting litter affets the design choices. While we could document what type of interaction the physical look and feel of the trash can encouraged, we also learned how crucial positioning the camera is and that gravity should be used when physically sorting trash into the sections corresponding to the categories defined in the code. We also learned how the realities of computer vision affect the possibilities of object categorisation.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 53,\n        columnNumber: 9\n      }\n    }, \"So, while a snappy pitch was prepared and the business model for the service approved by the jury, implementing this demo turned it into a feasible solution and a rewarding experience for our team.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Trash can-can\",\n      src: \"https://www.youtube.com/embed/cz3cQvwOsWA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 58,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 62,\n        columnNumber: 9\n      }\n    }, \"Mixed feelings\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 63,\n        columnNumber: 9\n      }\n    }, \"Exploring the magic of tangible interaction in mixed reality is hard to prototype in a very quick and dirty way. Me and my friend Joel did our best when sitting in the not-so-well air-conditioned homework room of our student accommodation building in France. Looking at the posters on the wall and drinking coke, we decided to create a portal to space and the ability to pass a virtual can of coke to a virtual astronaut by handing over the physical ones.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 72,\n        columnNumber: 9\n      }\n    }, \"Among with some other interactions, we implemented this with C# using the Unity engine with the Vuforia SDK. We put extra effort into the interactive and 3-dimensional sound design, which you unfortunately can't hear in this demo video which was filmed furing an exhibition, where headphones were used.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Trash can-can\",\n      src: \"https://www.youtube.com/embed/1jA0qqfTutA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 78,\n        columnNumber: 9\n      }\n    }));\n  }\n\n}","map":{"version":3,"sources":["/Users/kallior/Desktop/porto/src/Build.js"],"names":["React","Analyse","Component","render"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,WAAP;AACA,OAAO,sCAAP;AAEA,eAAe,MAAMC,OAAN,SAAsBD,KAAK,CAACE,SAA5B,CAAsC;AACnDC,EAAAA,MAAM,GAAG;AACP,wBACE;AAAK,MAAA,SAAS,EAAC,iBAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6eADF,eAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAXF,eAYE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,siBAZF,eAsBE;AACE,MAAA,KAAK,EAAC,MADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAtBF,eA0BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBA1BF,eA2BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iXA3BF,eAmCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0hBAnCF,eA6CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+MA7CF,eAkDE;AACE,MAAA,KAAK,EAAC,eADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAlDF,eAsDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAtDF,eAuDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,idAvDF,eAgEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wTAhEF,eAsEE;AACE,MAAA,KAAK,EAAC,eADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAtEF,CADF;AA6ED;;AA/EkD","sourcesContent":["import React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\n\nexport default class Analyse extends React.Component {\n  render() {\n    return (\n      <div className=\"Section-wrapper\">\n        <p>\n          Every product is a prototype. While the design process shouldn't begin\n          after putting a lot of effort in the implementation, it certainly\n          doesn't end there. The final product of a sprint or a project always\n          adds something new and unexpected, where lessons can be learned for\n          future reference. Some of the more explorative HCI projects I found\n          myself in during the college years were even based on this idea: the\n          human-computer interaction can only be validated once fully\n          experienced.\n        </p>\n        <h4>M:AV</h4>\n        <p>\n          This interactive art installation was born when we wanted to explore\n          immersion with movement-based human-computer interaction. Body\n          movement was mapped into digital sound and visuals using a Myo Gesture\n          Control Armband and the Max software. During an open ended exhibition,\n          we observed how people approached and immersed themselves in te\n          installation and organically received valuable insights about the\n          experience. These insights motivated a whole new round of ideation for\n          our future explorations in the movement-based interaction domain.\n        </p>\n        <iframe\n          title=\"M:AV\"\n          src=\"https://www.youtube.com/embed/lAoymyRFVdg\"\n        ></iframe>\n        <h4>Trash can-can</h4>\n        <p>\n          During an IoT hackathon in France, our team did a 3-day design sprint\n          with the following end result: an MVP for a self sorting trashcan. It\n          was powered by Raspberry Pi and based on computer vision, using the\n          OpenCV library for Python. During the live demo part of the pitch,\n          which included us acting out the use scenarios, it could recognize up\n          to 10 objects.\n        </p>\n        <p>\n          Building this MVP really helped us to consider how using computer\n          vision and code for self-sorting litter affets the design choices.\n          While we could document what type of interaction the physical look and\n          feel of the trash can encouraged, we also learned how crucial\n          positioning the camera is and that gravity should be used when\n          physically sorting trash into the sections corresponding to the\n          categories defined in the code. We also learned how the realities of\n          computer vision affect the possibilities of object categorisation.\n        </p>\n        <p>\n          So, while a snappy pitch was prepared and the business model for the\n          service approved by the jury, implementing this demo turned it into a\n          feasible solution and a rewarding experience for our team.\n        </p>\n        <iframe\n          title=\"Trash can-can\"\n          src=\"https://www.youtube.com/embed/cz3cQvwOsWA\"\n        ></iframe>\n        <h4>Mixed feelings</h4>\n        <p>\n          Exploring the magic of tangible interaction in mixed reality is hard\n          to prototype in a very quick and dirty way. Me and my friend Joel did\n          our best when sitting in the not-so-well air-conditioned homework room\n          of our student accommodation building in France. Looking at the\n          posters on the wall and drinking coke, we decided to create a portal\n          to space and the ability to pass a virtual can of coke to a virtual\n          astronaut by handing over the physical ones.\n        </p>\n        <p>\n          Among with some other interactions, we implemented this with C# using\n          the Unity engine with the Vuforia SDK. We put extra effort\n          into the interactive and 3-dimensional sound design, which you unfortunately can't hear in this\n          demo video which was filmed furing an exhibition, where headphones were used.\n        </p>\n        <iframe\n          title=\"Trash can-can\"\n          src=\"https://www.youtube.com/embed/1jA0qqfTutA\"\n        ></iframe>\n      </div>\n    );\n  }\n}\n"]},"metadata":{},"sourceType":"module"}