{"ast":null,"code":"var _jsxFileName = \"/Users/kallior/Desktop/porto/src/Build.js\";\nimport React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\nexport default class Analyse extends React.Component {\n  render() {\n    return /*#__PURE__*/React.createElement(\"div\", {\n      className: \"Section-wrapper\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 9,\n        columnNumber: 9\n      }\n    }, \"Every product is a prototype. While the design process shouldn't begin after putting a lot of effort in the implementation, it certainly doesn't end there. The final product of a sprint or a project always adds something new and unexpected, where lessons can be learned for future reference. Some of the more explorative HCI projects I found myself in during the college years were even based on this idea: the human-computer interaction can only be validated once fully experienced.\"), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 19,\n        columnNumber: 9\n      }\n    }, \"M:AV\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 20,\n        columnNumber: 9\n      }\n    }, \"This interactive art installation was born when we wanted to explore immersion with movement-based human-computer interaction. Body movement was mapped into digital sound and visuals using a Myo Gesture Control Armband and the Max software. During an open ended exhibition, we observed how people approached and immersed themselves in te installation and organically received valuable insights about the experience. These insights motivated a whole new round of ideation for our future explorations in the movement-based interaction domain.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"M:AV\",\n      src: \"https://www.youtube.com/embed/lAoymyRFVdg\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 30,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 34,\n        columnNumber: 9\n      }\n    }, \"Trash can-can\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 35,\n        columnNumber: 9\n      }\n    }, \"During an IoT hackathon in France, our team did a 3-day design sprint with the following end result: an MVP for a self sorting trashcan. It was powered by Raspberry Pi and based on computer vision, using the OpenCV library for Python. During the live demo part of the pitch, which included us acting out the use scenarios, it could recognize up to 10 objects.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 43,\n        columnNumber: 9\n      }\n    }, \"Building this MVP really helped us to consider how using computer vision and code for self-sorting litter affets the design choices. While we could document what type of interaction the physical look and feel of the trash can encouraged, we also learned how crucial positioning the camera is and that gravity should be used when physically sorting trash into the sections corresponding to the categories defined in the code. We also learned how the realities of computer vision affect the possibilities of object categorisation.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 53,\n        columnNumber: 9\n      }\n    }, \"So, while a snappy pitch was prepared and the business model for the service approved by the jury, implementing this demo turned it into a feasible solution and a rewarding experience for our team.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Trash can-can\",\n      src: \"https://www.youtube.com/embed/cz3cQvwOsWA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 58,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 62,\n        columnNumber: 9\n      }\n    }, \"Mixed feelings\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 63,\n        columnNumber: 9\n      }\n    }, /*#__PURE__*/React.createElement(\"img\", {\n      src: require(\"./images/mixed_feelings.png\"),\n      alt: \"Mixed feelings\",\n      className: \"Image medium left\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 64,\n        columnNumber: 11\n      }\n    }), \"Exploring the magic of tangible interaction in mixed reality is hard to prototype in a very quick and dirty way. Me and my friend Joel did our best when sitting in the not-so-well air-conditioned homework room of our student accommodation building in France. Looking at the posters on the wall and drinking coke, we decided to create a portal to space and the ability to pass a virtual can of coke to a virtual astronaut by handing over the physical ones. The portal could be \\\"opened\\\" through a smartphone.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 78,\n        columnNumber: 9\n      }\n    }, \"Among with some other interactions, we implemented this with C# using the Unity engine with the Vuforia SDK. We put quite some effort into the interactive and 3-dimensional sound design of this reality, which you unfortunately can't appreciate in the following video which was filmed during an open exhibition, where headphones were used. You can, however, witness how the virtual characters respond to the tangible interaction of the user by passing through the portal, further seeming to bridge the gap between the two realities.\"), /*#__PURE__*/React.createElement(\"iframe\", {\n      title: \"Mixed feelings\",\n      src: \"https://www.youtube.com/embed/1jA0qqfTutA\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 88,\n        columnNumber: 9\n      }\n    }), /*#__PURE__*/React.createElement(\"h4\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 92,\n        columnNumber: 9\n      }\n    }, \"Web apps\"), /*#__PURE__*/React.createElement(\"h5\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 93,\n        columnNumber: 9\n      }\n    }, \"BFAT\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 94,\n        columnNumber: 9\n      }\n    }, \"I market myself as a designer whose empathy doesn't end with the end user. Because I've done agile full-stack development, I'm familiar with the workflow defined by two screens, a keyboard and coffee breaks. BFAT 3 was a test automation solution me and my team at Nokia built from scratch and released on time during a year between 2017 and 2018. It was implemented with React, Node.js and MongoDB.\"), /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 102,\n        columnNumber: 11\n      }\n    }, \"I learned a great deal about software architecture and the ways in which the GUI actually connects to (and is defined by) the many underlying data structures, that are more than just callbacks on the server side. On the other hand, I finally and fully realized how the GUI is, connected to the user experience and how immensely useful a design system (a customized bootstrap library) could be with the lack of a separate UI and graphic designer.\", \" \"), /*#__PURE__*/React.createElement(\"h5\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 111,\n        columnNumber: 9\n      }\n    }, \"Hands and Omens website\"));\n  }\n\n}","map":{"version":3,"sources":["/Users/kallior/Desktop/porto/src/Build.js"],"names":["React","Analyse","Component","render","require"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,WAAP;AACA,OAAO,sCAAP;AAEA,eAAe,MAAMC,OAAN,SAAsBD,KAAK,CAACE,SAA5B,CAAsC;AACnDC,EAAAA,MAAM,GAAG;AACP,wBACE;AAAK,MAAA,SAAS,EAAC,iBAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6eADF,eAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAXF,eAYE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,siBAZF,eAsBE;AACE,MAAA,KAAK,EAAC,MADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAtBF,eA0BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBA1BF,eA2BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iXA3BF,eAmCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0hBAnCF,eA6CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+MA7CF,eAkDE;AACE,MAAA,KAAK,EAAC,eADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAlDF,eAsDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAtDF,eAuDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE;AACE,MAAA,GAAG,EAAEC,OAAO,CAAC,6BAAD,CADd;AAEE,MAAA,GAAG,EAAC,gBAFN;AAGE,MAAA,SAAS,EAAC,mBAHZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADF,igBAvDF,eAsEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6hBAtEF,eAgFE;AACE,MAAA,KAAK,EAAC,gBADR;AAEE,MAAA,GAAG,EAAC,2CAFN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAhFF,eAoFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBApFF,eAqFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cArFF,eAsFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wZAtFF,eA8FI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wcAOkC,GAPlC,CA9FJ,eAuGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iCAvGF,CADF;AA2GD;;AA7GkD","sourcesContent":["import React from \"react\";\nimport \"./App.css\";\nimport \"bootstrap/dist/css/bootstrap.min.css\";\n\nexport default class Analyse extends React.Component {\n  render() {\n    return (\n      <div className=\"Section-wrapper\">\n        <p>\n          Every product is a prototype. While the design process shouldn't begin\n          after putting a lot of effort in the implementation, it certainly\n          doesn't end there. The final product of a sprint or a project always\n          adds something new and unexpected, where lessons can be learned for\n          future reference. Some of the more explorative HCI projects I found\n          myself in during the college years were even based on this idea: the\n          human-computer interaction can only be validated once fully\n          experienced.\n        </p>\n        <h4>M:AV</h4>\n        <p>\n          This interactive art installation was born when we wanted to explore\n          immersion with movement-based human-computer interaction. Body\n          movement was mapped into digital sound and visuals using a Myo Gesture\n          Control Armband and the Max software. During an open ended exhibition,\n          we observed how people approached and immersed themselves in te\n          installation and organically received valuable insights about the\n          experience. These insights motivated a whole new round of ideation for\n          our future explorations in the movement-based interaction domain.\n        </p>\n        <iframe\n          title=\"M:AV\"\n          src=\"https://www.youtube.com/embed/lAoymyRFVdg\"\n        ></iframe>\n        <h4>Trash can-can</h4>\n        <p>\n          During an IoT hackathon in France, our team did a 3-day design sprint\n          with the following end result: an MVP for a self sorting trashcan. It\n          was powered by Raspberry Pi and based on computer vision, using the\n          OpenCV library for Python. During the live demo part of the pitch,\n          which included us acting out the use scenarios, it could recognize up\n          to 10 objects.\n        </p>\n        <p>\n          Building this MVP really helped us to consider how using computer\n          vision and code for self-sorting litter affets the design choices.\n          While we could document what type of interaction the physical look and\n          feel of the trash can encouraged, we also learned how crucial\n          positioning the camera is and that gravity should be used when\n          physically sorting trash into the sections corresponding to the\n          categories defined in the code. We also learned how the realities of\n          computer vision affect the possibilities of object categorisation.\n        </p>\n        <p>\n          So, while a snappy pitch was prepared and the business model for the\n          service approved by the jury, implementing this demo turned it into a\n          feasible solution and a rewarding experience for our team.\n        </p>\n        <iframe\n          title=\"Trash can-can\"\n          src=\"https://www.youtube.com/embed/cz3cQvwOsWA\"\n        ></iframe>\n        <h4>Mixed feelings</h4>\n        <p>\n          <img\n            src={require(\"./images/mixed_feelings.png\")}\n            alt=\"Mixed feelings\"\n            className=\"Image medium left\"\n          />\n          Exploring the magic of tangible interaction in mixed reality is hard\n          to prototype in a very quick and dirty way. Me and my friend Joel did\n          our best when sitting in the not-so-well air-conditioned homework room\n          of our student accommodation building in France. Looking at the\n          posters on the wall and drinking coke, we decided to create a portal\n          to space and the ability to pass a virtual can of coke to a virtual\n          astronaut by handing over the physical ones. The portal could be\n          \"opened\" through a smartphone.\n        </p>\n        <p>\n          Among with some other interactions, we implemented this with C# using\n          the Unity engine with the Vuforia SDK. We put quite some effort into\n          the interactive and 3-dimensional sound design of this reality, which\n          you unfortunately can't appreciate in the following video which was\n          filmed during an open exhibition, where headphones were used. You can,\n          however, witness how the virtual characters respond to the tangible\n          interaction of the user by passing through the portal, further seeming\n          to bridge the gap between the two realities.\n        </p>\n        <iframe\n          title=\"Mixed feelings\"\n          src=\"https://www.youtube.com/embed/1jA0qqfTutA\"\n        ></iframe>\n        <h4>Web apps</h4>\n        <h5>BFAT</h5>\n        <p>\n          I market myself as a designer whose empathy doesn't end with the end\n          user. Because I've done agile full-stack development, I'm familiar\n          with the workflow defined by two screens, a keyboard and coffee\n          breaks. BFAT 3 was a test automation solution me and my team at Nokia\n          built from scratch and released on time during a year between\n          2017 and 2018. It was implemented with React, Node.js and MongoDB. \n          </p>\n          <p>\n          I learned a great deal about software architecture and the ways in which\n          the GUI actually connects to (and is defined by) the many underlying\n          data structures, that are more than just callbacks on the server side.\n          On the other hand, I finally and fully realized how the GUI is,\n          connected to the user experience and how immensely useful a design\n          system (a customized bootstrap library) could be with the lack of a\n          separate UI and graphic designer.{\" \"}\n        </p>\n        <h5>Hands and Omens website</h5>\n      </div>\n    );\n  }\n}\n"]},"metadata":{},"sourceType":"module"}